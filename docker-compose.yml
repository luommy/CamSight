version: '3.8'

# ==============================================================================
# Live VLM WebUI + Ollama
# ==============================================================================
# This is a simpler alternative to vLLM, great for quick testing and demos.
# Ollama provides an easy-to-use interface for running vision models locally.
#
# Usage:
#   # PC (x86_64)
#   docker compose -f docker-compose.ollama.yml --profile live-vlm-webui-x86 up
#
#   # Jetson Orin
#   docker compose -f docker-compose.ollama.yml --profile live-vlm-webui-jetson-orin up
#
#   # Jetson Thor
#   docker compose -f docker-compose.ollama.yml --profile live-vlm-webui-jetson-thor up
#
# After starting, pull a vision model:
#   docker exec ollama ollama pull llama3.2-vision:11b
#
# Access:
#   - Live VLM WebUI: https://localhost:8090
#   - Ollama API: http://localhost:11434/v1
# ==============================================================================

services:
  # ============================================================================
  # Ollama Service
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - vlm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # Live VLM WebUI - PC (x86_64)
  # ============================================================================
  live-vlm-webui-x86:
    image: ghcr.io/nvidia-ai-iot/live-vlm-webui:latest-x86
    container_name: live-vlm-webui
    profiles:
      - live-vlm-webui-x86
    network_mode: host
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]

  # ============================================================================
  # Live VLM WebUI - Jetson Orin
  # ============================================================================
  live-vlm-webui-jetson-orin:
    image: ghcr.io/nvidia-ai-iot/live-vlm-webui:latest-jetson-orin
    container_name: live-vlm-webui
    profiles:
      - live-vlm-webui-jetson-orin
    network_mode: host
    privileged: true
    volumes:
      - /run/jtop.sock:/run/jtop.sock:ro
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    runtime: nvidia

  # ============================================================================
  # Live VLM WebUI - Jetson Thor
  # ============================================================================
  live-vlm-webui-jetson-thor:
    image: ghcr.io/nvidia-ai-iot/live-vlm-webui:latest-jetson-thor
    container_name: live-vlm-webui
    profiles:
      - live-vlm-webui-jetson-thor
    network_mode: host
    privileged: true
    volumes:
      - /run/jtop.sock:/run/jtop.sock:ro
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]

volumes:
  ollama-data:
    driver: local

networks:
  vlm-network:
    driver: bridge

# ==============================================================================
# Quick Start:
# ==============================================================================
# 1. Start services:
#    docker compose -f docker-compose.ollama.yml --profile live-vlm-webui-x86 up -d
#
# 2. Pull a vision model:
#    docker exec ollama ollama pull llama3.2-vision:11b
#    # or
#    docker exec ollama ollama pull llava:7b
#
# 3. Access Web UI:
#    https://localhost:8090
#
# 4. Select model in UI and start camera!
# ==============================================================================

